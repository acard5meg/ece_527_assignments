{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Exercise #02 - The Perceptron Learning Algorithm ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Included below is the core code for this computer exercise.  You will need set the parameters of the classes according to what you would like to do.  If you accept the default parameters, make sure you understand what they are, and make sure you understand what parameters may be set.\n",
    "\n",
    "For repeatable experiments, you may want to consider setting the random number seed when generating datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple percepton learning algorithm that you may modify to perform the pocket algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pocket_perceptron(X, y_enter, eta = 1, eps = 20):\n",
    "\n",
    "    # Recoding instances of 0 to -1 so the \n",
    "    # range of target values is +-1\n",
    "    y = np.where(y_enter == 0, -1, 1)\n",
    "\n",
    "    def emp_risk():\n",
    "        risk = 0\n",
    "        for i, x in enumerate(X):\n",
    "            risk += max(0, -y[i]*np.dot(X[i], w))\n",
    "        return risk\n",
    "    \n",
    "    w = np.zeros(len(X[0]))\n",
    "    eta = eta\n",
    "    epochs = eps\n",
    "    pocket, pocket_error = w, float('inf')\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        for i, x in enumerate(X):\n",
    "            if (np.dot(X[i], w)*y[i]) <= 0:\n",
    "                w = w + eta*X[i]*y[i]\n",
    "                test_error = emp_risk()\n",
    "                if test_error < pocket_error:\n",
    "                    pocket_error = test_error\n",
    "                    pocket = w\n",
    "    return pocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, weights, dataset_num, data_type):\n",
    "    # Generate a grid of points\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), \n",
    "                         np.linspace(y_min, y_max, 500))\n",
    "\n",
    "    # Compute the decision boundary\n",
    "    Z = np.dot(np.c_[xx.ravel(), yy.ravel()], weights)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Save figure in working directory\n",
    "    plt.title(f\"Decision Boundary Dataset: {dataset_num}\\n{data_type.title()} Algorithm Method\")\n",
    "    \n",
    "    \n",
    "    # Plot decision boundary (contour line) if there is a clear separation\n",
    "    plt.contour(xx, yy, Z, levels=[0], colors='black', linestyles='--')\n",
    "\n",
    "    # Plot the points\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend(handles=scatter.legend_elements()[0], labels=['Class -1', 'Class 1'])\n",
    "\n",
    "    # Set plot title and labels\n",
    "    \n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.savefig(f\"dec_bound_{dataset_num}_{data_type}.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from first feature take all elements\n",
    "# from second feature take elements 0\n",
    "\n",
    "\n",
    "# X1[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.rand(100,2)\n",
    "y1 = (-0.5*X1[:,0] + X1[:,1] -0.25 > 0).astype(int)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.scatter(X1[:, 0], X1[:, 1], c=y1, alpha=0.9, edgecolors='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question p.4: PLA target values need to be converted to $y = \\pm1$. This is because PLA is based on stochastic gradient descent and the solution update, shown below, would become zero for observations where y is equal to 0. The $\\pm1$ is to allow the update to move in the correct direction after a classifcation error.\n",
    "$$\\overrightarrow{w_{n+1}} = \\overrightarrow{w_n} + \\mu*y_k*\\overrightarrow{x_k}$$\n",
    "\n",
    "The Perceptron class doesn't care what the target values are. However, the target values need to be in a binary format to allow the Perceptron class to function properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Perceptron()\n",
    "x_train, x_test, y_train, y_test = train_test_split(X1, y1, test_size = 0.25)\n",
    "print(f\"min: {min(y1)} and max: {max(y1)}\")\n",
    "clf.fit(x_train,y_train)\n",
    "clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set #2\n",
    "Shown along with a scatter plot is the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.random.rand(100,2)\n",
    "y2 = (X2[:,1] > 4*(X2[:,0]-0.5)**2).astype(int)\n",
    "plt.scatter(X2[:, 0], X2[:, 1], c=y2, alpha=0.9, edgecolors='black')\n",
    "#Plot decision boundary\n",
    "x1 = np.linspace(0, 1, 400)\n",
    "x2 = 4 * (x1 - 0.5) ** 2\n",
    "plt.plot(x1, x2, color='green')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X3,y3 = make_moons(n_samples=100, shuffle=True, noise=0.1)\n",
    "plt.scatter(X3[:, 0], X3[:, 1], c=y3, alpha=0.9, edgecolors='black')\n",
    "plt.grid(True)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X4,y4 = make_circles(n_samples=100, shuffle=True, noise=0.1,factor=0.5)\n",
    "plt.scatter(X4[:, 0], X4[:, 1], c=y4, alpha=0.9, edgecolors='black')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Test Sets\n",
    "\n",
    "You may want to put in your own value for `test_size` instead of accepting the default value.\n",
    "\n",
    "### Important Note ###\n",
    "In the `train_test_splot` class you will need to put in the desired dataset, e.g. `(X1,y1)` or `(X2,y2)` in the place of `(X,y)`, or redefine `(X,y)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLA Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1\n",
    "\n",
    "To do:\n",
    "1. Scatter plot of test set and discriminant function\n",
    "2. Plot of learning curve\n",
    "3. Expected risk of classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pocket_error(x_train, x_test, y_train, y_test):\n",
    "    pocket_coef = pocket_perceptron(x_train, y_train)\n",
    "    print(pocket_coef)\n",
    "    y_guess_pocket = np.where(np.dot(x_test,pocket_coef)>0, 1, 0)\n",
    "    y_pocket_sum = np.sum(y_guess_pocket)\n",
    "    y_test_sum = np.sum(y_test)\n",
    "    print(f\"Pocket error: {abs(y_pocket_sum - y_test_sum) / len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve_plot_pla(data_num, epochs, train_x, test_x, train_y, test_y,  tol = 1e-3):\n",
    "    learning = []\n",
    "    for i in range(1, epochs+1):\n",
    "        learning_perceptron = Perceptron(max_iter = i, tol = tol)\n",
    "        learning_perceptron.fit(train_x, train_y)\n",
    "        learning.append(1 - learning_perceptron.score(test_x, test_y))\n",
    "    learning = np.array(learning)\n",
    "    plt.title(f\"Classification Error Dataset Number: {data_num}\")\n",
    "    plt.axis([1,epochs,0,1])\n",
    "    plt.xticks(np.arange(1,epochs+1,1))\n",
    "    plt.yticks(np.arange(0,1.1,0.1))\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.plot(learning)\n",
    "    plt.savefig(f\"pla_learning_curve_d_{data_num}.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.51106848  0.51055049]\n",
      "Pocket error: 0.05\n",
      "1 epoch: [-5.11068478e-05  5.10550488e-05]\n",
      "[[-2.22493757  4.12214274]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(23)\n",
    "X1 = np.random.rand(100,2)\n",
    "y1 = (-0.5*X1[:,0] + X1[:,1] -0.25 > 0).astype(int)\n",
    "\n",
    "# Dataset #1\n",
    "# test size of 20%\n",
    "# tol set to None, done because this dataset is linearly separable so convergence theorem gurantees PLA will converge; however \n",
    "# this decision suffers from data snooping since I've seen the data. When I did this the PLA always used max_iter as a stopping\n",
    "# criterion so the PLA didn't converge. Set tol to 1e-12 because the PLA should converge and setting a lower threshold approximates\n",
    "# convergence and it converges after 13 epochs.\n",
    "# Left eta0 at default value because convergence theorem states that we'll theoretically have convergence with any step size\n",
    "# Running this with different random seeds changes the accuracy of the model on both the training and test sets; however, the \n",
    "# model still converges. For example, a random seed of 7 results in error rates of 0.2 and 0.05 for the test and training data, respectively.\n",
    "# This could be an instance where the stopping condition is kicking in too early because if I change tol to None the training and test\n",
    "# error are reduced to 0, but the number of epochs increases to the max_iter. \n",
    "# Switching tol from 1e-12 to 1 reduces the number of epochs to 6 and increases the test accuracy to 95%\n",
    "\n",
    "x1_pla_train, x1_pla_test, y1_pla_train, y1_pla_test = train_test_split(X1, y1, test_size = 0.2)\n",
    "pla_1 = Perceptron(tol = 1e-12)\n",
    "pla_1.fit(x1_pla_train, y1_pla_train)\n",
    "# print(pla_1.n_iter_)\n",
    "# print(pla_1.score(x1_pla_test,y1_pla_test))\n",
    "# print(pla_1.score(x1_pla_train,y1_pla_train))\n",
    "pocket_error(x1_pla_train, x1_pla_test, y1_pla_train, y1_pla_test)\n",
    "print(f\"1 epoch: {pocket_perceptron(x1_pla_train, y1_pla_train,0.0001,1)}\")\n",
    "print(pla_1.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_plot_pla(1, 20, x1_pla_train, x1_pla_test, y1_pla_train, y1_pla_test, 1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(X1,y1, pla_1.coef_[0], 1, \"perceptron\")\n",
    "plot_decision_boundary(X1,y1, pocket_perceptron(x1_pla_train, y1_pla_train), 1, \"pocket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2\n",
    "\n",
    "To do:\n",
    "1. Rerun after setting tol=None then answer #2 subquestions\n",
    "2. Plot learning curve\n",
    "3. Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X2 = np.random.rand(100,2)\n",
    "y2 = (X2[:,1] > 4*(X2[:,0]-0.5)**2).astype(int)\n",
    "\n",
    "# Dataset #2\n",
    "# Made the test size 25% of the data\n",
    "# We know by the data construction the data are not linearly separable, therefore, PLA will never converge theoretically\n",
    "# I kept the default value of the stopping criterion, tol, because we know PLA will not converge, however, I increased the\n",
    "# max number of iterations to see if the default stopping criterion would result in a premature exit from the SGD.\n",
    "# USing the default value of the stopping criterion resulted in the model converging which shouldn't happen in theory; therefore,\n",
    "# I reduced tol to a smaller number, but max_iter was never reached.\n",
    "# Surprisingly, PLA converged after 6 epochs.\n",
    "# The accuracy of PLA was 0.63 and 0.8 on the test and training data, respectively. The small accuracy suggests we need to \n",
    "# try a non-linear model\n",
    "# Changing the random seed changes the number of epochs and the accuracy of the model on both the training and test data; however, \n",
    "# it doesn't fix the underwhelming performance of PLA on the test data \n",
    "\n",
    "x2_pla_train, x2_pla_test, y2_pla_train, y2_pla_test = train_test_split(X2, y2, test_size = 0.25)\n",
    "pla_2 = Perceptron(max_iter = 100000)\n",
    "pla_2.fit(x2_pla_train, y2_pla_train)\n",
    "print(pla_2.n_iter_)\n",
    "print(pla_2.score(x2_pla_test, y2_pla_test))\n",
    "print(pla_2.score(x2_pla_train, y2_pla_train))\n",
    "pocket_error(x2_pla_train, x2_pla_test, y2_pla_train, y2_pla_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_plot_pla(2, 20, x2_pla_train, x2_pla_test, y2_pla_train, y2_pla_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #3\n",
    "To do:\n",
    "1. Rerun to avoid convergence and answer #2 subparts\n",
    "2. Scatterplots\n",
    "3. Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X3,y3 = make_moons(n_samples=100, shuffle=True, noise=0.1)\n",
    "\n",
    "# Dataset #3\n",
    "# Using test dataset size of 0.2\n",
    "# The data are not linearly separable therefore, theoretically, PLA should not converge\n",
    "# Initially, I set max_iter to 10000 expecting PLA not to converge and left tol as the default. This was not the case\n",
    "# as PLA converged in 7 epochs with a training score of 0.85. Afterward, I decreased tol to 1e-10 and reran the model, but\n",
    "# was met with convergence in the same number of epochs and the same training accuracy\n",
    "# Running PLA with different random seeds changing the number of epochs required for convergence and the mdoel accuracy\n",
    "# but did not fundamentally change anything\n",
    "# The training and test accuracy were identical at 0.85\n",
    "\n",
    "x3_pla_train, x3_pla_test, y3_pla_train, y3_pla_test = train_test_split(X3, y3, test_size = 0.2)\n",
    "pla_3 = Perceptron(max_iter = 10000, tol = 1e-10)\n",
    "pla_3.fit(x3_pla_train, y3_pla_train)\n",
    "print(pla_3.n_iter_)\n",
    "print(pla_3.score(x3_pla_test, y3_pla_test))\n",
    "print(pla_3.score(x3_pla_train, y3_pla_train))\n",
    "pocket_error(x3_pla_train, x3_pla_test, y3_pla_train, y3_pla_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_plot_pla(3, 20, x3_pla_train, x3_pla_test, y3_pla_train, y3_pla_test, 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #4\n",
    "\n",
    "To do\n",
    "1. Rerun to avoid convergence and answer #2 subparts\n",
    "2. Scatterplots\n",
    "3. Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X4,y4 = make_circles(n_samples=100, shuffle=True, noise=0.1,factor=0.5)\n",
    "\n",
    "# Dataset #4\n",
    "# Using default test split\n",
    "# We know from the data construction the data are not linearly separable and they will not converge in theory\n",
    "# Begin by setting max iter to 10,000 under the assumption there will not be convergence so we'll let the \n",
    "# algorithm run.\n",
    "# The algorithm converges but with test accuracy of 0.28 and a train accuracy of 0.55. These abysmal test results indicate\n",
    "# the linear perceptron model is not suited for the underlying data and we should build a new model. Changing the random seed\n",
    "# causes the number of epochs and accuracy to change slightly, but the conclusion remains the model isn't suited for the data\n",
    "\n",
    "x4_pla_train, x4_pla_test, y4_pla_train, y4_pla_test = train_test_split(X4, y4)\n",
    "pla_4 = Perceptron(max_iter = 10000, tol = 0.00001)\n",
    "pla_4.fit(x4_pla_train, y4_pla_train)\n",
    "print(pla_4.n_iter_)\n",
    "print(pla_4.score(x4_pla_test, y4_pla_test))\n",
    "print(pla_4.score(x4_pla_train, y4_pla_train))\n",
    "pocket_error(x4_pla_train, x4_pla_test, y4_pla_train, y4_pla_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_plot_pla(4, 20, x4_pla_train, x4_pla_test, y4_pla_train, y4_pla_test, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(X4,y4, pla_4.coef_[0], 4, \"perceptron\")\n",
    "plot_decision_boundary(X4,y4, pocket_perceptron(x4_pla_train, y4_pla_train), 4, \"pocket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear PLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting a Feature Vector\n",
    "\n",
    "Here is an example of adding polynomial features, $x_1^2$, $x_1x_2$, and $x_2^2$ to the feature vector **x** to create a feature vector of length 5 plus the bias.  For a higher-order polynomial, change the value of `degree`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1 Non-linear PLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "X1 = np.random.rand(100,2)\n",
    "y1 = (-0.5*X1[:,0] + X1[:,1] -0.25 > 0).astype(int)\n",
    "\n",
    "x1_pla_train, x1_pla_test, y1_pla_train, y1_pla_test = train_test_split(X1, y1, test_size = 0.2)\n",
    "\n",
    "# Dataset 1, keeping degree to 2 because data is linearly separable so non-linear classifier inappropriate\n",
    "# PLA converges after 20 epochs and the accuracy of the model is 0.95 and 0.9875 for test and training data respectively.\n",
    "# This construction takes more epochs to converge, but has higher test accuracy than the linear model using the same random seed\n",
    "# Using different random number seeds changes the number of epochs and errors, but doesn't fundamentally change the results.\n",
    "# Using a highly nonlinear perceptron, degrees = 10 results in 14 epochs and a training accuracy of 0.9.\n",
    "\n",
    "\n",
    "poly_pla_1 = PolynomialFeatures(degree=2, include_bias=True)\n",
    "x1_poly_pla_train = poly_pla_1.fit_transform(x1_pla_train)\n",
    "x1_poly_pla_test = poly_pla_1.fit_transform(x1_pla_test)\n",
    "\n",
    "perceptron_poly_1 = Perceptron(tol = 1e-9)\n",
    "perceptron_poly_1.fit(x1_poly_pla_train, y1_pla_train)\n",
    "\n",
    "print(perceptron_poly_1.n_iter_)\n",
    "print(perceptron_poly_1.score(x1_poly_pla_test, y1_pla_test))\n",
    "print(perceptron_poly_1.score(x1_poly_pla_train, y1_pla_train))\n",
    "pocket_error(x1_poly_pla_train, x1_poly_pla_test, y1_pla_train, y1_pla_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2 Non-linear PLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X2 = np.random.rand(100,2)\n",
    "y2 = (X2[:,1] > 4*(X2[:,0]-0.5)**2).astype(int)\n",
    "\n",
    "x2_pla_train, x2_pla_test, y2_pla_train, y2_pla_test = train_test_split(X2, y2, test_size = 0.25)\n",
    "\n",
    "# Dataset # 2\n",
    "# Kept the polynomial features at a degree of 2 because we know the data are constructed quadratically\n",
    "# Increased the number of epoch stopping condition, reduced the stopping criterion, and reduced the multiplier. All of\n",
    "# this was done to better fit the quadratic data\n",
    "# With the quadratic data I end up converging in 6 epochs, with a higher test accuracy than training, but the results\n",
    "# are equivalent to the linear classifier, 0.8 test accuracy\n",
    "# Increasing the Polynomial features degrees to 3 increases the test accuracy to 0.88\n",
    "\n",
    "poly_pla_2 = PolynomialFeatures(degree=2, include_bias=True)\n",
    "x2_train_poly_pla_2, x2_test_poly_pla_2 = poly_pla_2.fit_transform(x2_pla_train), poly_pla_2.fit_transform(x2_pla_test)\n",
    "\n",
    "perceptron_poly_2 = Perceptron(tol = 1e-6, max_iter = 10000, eta0 = 0.000001, n_iter_no_change = 370)\n",
    "perceptron_poly_2.fit(x2_train_poly_pla_2, y2_pla_train)\n",
    "\n",
    "print(perceptron_poly_2.n_iter_)\n",
    "print(perceptron_poly_2.score(x2_test_poly_pla_2, y2_pla_test))\n",
    "print(perceptron_poly_2.score(x2_train_poly_pla_2, y2_pla_train))\n",
    "pocket_error(x2_train_poly_pla_2, x2_test_poly_pla_2, y2_pla_train, y2_pla_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3 Non-Linear PLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X3,y3 = make_moons(n_samples=100, shuffle=True, noise=0.1)\n",
    "\n",
    "# Dataset #3\n",
    "# Using test dataset size of 0.2\n",
    "# The data are not linearly separable therefore, linear PLA should not converge\n",
    "# Trying polynomial features of 3 and 5. Knowing the structure of the data beforehand, violating the data snooping \n",
    "# principle, the data seem to be shaped in this fashion.\n",
    "# Leaving the perceptron with all default parameters\n",
    "# With polynomial of 3 the training and test accuracy were 1 and the algorithm ran 15 epochs\n",
    "# Changing the random seed does little to alter this conclusion\n",
    "# Using degree of 5 drops the test accuracy to 0.95 and the number of epochs to 14\n",
    "\n",
    "x3_pla_train, x3_pla_test, y3_pla_train, y3_pla_test = train_test_split(X3, y3, test_size = 0.2)\n",
    "poly_pla_3 = PolynomialFeatures(degree=3, include_bias = True)\n",
    "x3_train_poly_pla3, x3_test_poly_pla3 = poly_pla_3.fit_transform(x3_pla_train), poly_pla_3.fit_transform(x3_pla_test)\n",
    "\n",
    "perceptron_poly_3 = Perceptron()\n",
    "perceptron_poly_3.fit(x3_train_poly_pla3, y3_pla_train)\n",
    "\n",
    "print(perceptron_poly_3.n_iter_)\n",
    "print(perceptron_poly_3.score(x3_test_poly_pla3, y3_pla_test))\n",
    "print(perceptron_poly_3.score(x3_train_poly_pla3, y3_pla_train))\n",
    "pocket_error(x3_train_poly_pla3, x3_test_poly_pla3, y3_pla_train, y3_pla_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 4 Non-Linear PLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X4,y4 = make_circles(n_samples=100, shuffle=True, noise=0.1,factor=0.5)\n",
    "\n",
    "# Dataset #4\n",
    "# Using default test split\n",
    "# Knowing the data are circles I will keep the perceptron default values\n",
    "# Using polynomial degrees from 2-6 result in test accuracies of 1\n",
    "\n",
    "\n",
    "x4_pla_train, x4_pla_test, y4_pla_train, y4_pla_test = train_test_split(X4, y4)\n",
    "\n",
    "to_df = {'degrees':[2,3,4,5,6],\n",
    "         'test accuracy':[],\n",
    "         'train accuracy':[],\n",
    "         'epochs':[], \n",
    "         'pocket error':[]\n",
    "        }\n",
    "\n",
    "for i in range(2,7):\n",
    "    poly_pla_4 = PolynomialFeatures(degree=i, include_bias = True)\n",
    "    x4_train_poly_pla_4, x4_test_poly_pla_4 = poly_pla_4.fit_transform(x4_pla_train), poly_pla_4.fit_transform(x4_pla_test)\n",
    "    perceptron_poly_4 = Perceptron()\n",
    "    perceptron_poly_4.fit(x4_train_poly_pla_4, y4_pla_train)\n",
    "    \n",
    "    \n",
    "    to_df['test accuracy'].append(perceptron_poly_4.score(x4_test_poly_pla_4, y4_pla_test))\n",
    "    to_df['train accuracy'].append(perceptron_poly_4.score(x4_train_poly_pla_4, y4_pla_train))\n",
    "    to_df['epochs'].append(perceptron_poly_4.n_iter_)\n",
    "    to_df['pocket error'].append(pocket_error(x4_train_poly_pla_4, x4_test_poly_pla_4, y4_pla_train, y4_pla_test))\n",
    "table = pd.DataFrame(to_df)\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shoelace Theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shoelace_area(x1, y1, x2, y2, x3, y3):\n",
    "    # Calculate the area using the Shoelace formula\n",
    "    area = abs(x1 * y2 + x2 * y3 + x3 * y1 - (y1 * x2 + y2 * x3 + y3 * x1)) / 2\n",
    "    return area"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
